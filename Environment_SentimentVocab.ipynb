{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk import download as nltk_download\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import random\n",
    "import seaborn as sns\n",
    "import eng_spacysentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-helping",
   "metadata": {},
   "source": [
    "# Парсинг новостей по экологии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pages_climate = []\n",
    "all_pages_pollution = []\n",
    "all_pages_energy = []\n",
    "all_pages_wildlife = []\n",
    "\n",
    "base_url = \"https://www.theguardian.com/environment/\"\n",
    "\n",
    "for i in range(1,16):\n",
    "    all_pages_climate.append(f'{base_url}climate-crisis?page={i}')\n",
    "for i in range(1,16):\n",
    "    all_pages_pollution.append(f'{base_url}pollution?page={i}')\n",
    "for i in range(1,16):\n",
    "    all_pages_energy.append(f'{base_url}energy?page={i}')\n",
    "for i in range(1,16):\n",
    "    all_pages_wildlife.append(f'{base_url}wildlife?page={i}')\n",
    "    \n",
    "\n",
    "#Сбор ссылок на страницы сайта, содержащие ссылки на новости разделов (climate, pollution, energy, wildlife), \n",
    "#по 15 страниц на каждый раздел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetLinks(all_pages):\n",
    "    \"\"\"\n",
    "    Сбор ссылок на новости в текстовом формате (то есть без ссылок на новоти в формате видео/аудио/фото/интерактивов)\n",
    "    \"\"\"\n",
    "    \n",
    "    all_links = []\n",
    "    for page in all_pages:\n",
    "        link = rq.get(page)\n",
    "        soup = BeautifulSoup(link.text, features=\"html.parser\") \n",
    "        for a in soup.find_all(\"a\", class_=\"u-faux-block-link__overlay js-headline-text\"):\n",
    "            if 'video' not in a.get('href') and \\\n",
    "            'audio' not in a.get('href') and \\\n",
    "            'gallery' not in a.get('href') and \\\n",
    "            'commentisfree' not in a.get('href') and \\\n",
    "            'ng-interactive' not in a.get('href'):\n",
    "                all_links.append(a.get('href'))\n",
    "                \n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetNews(url):\n",
    "    \"\"\"\n",
    "    Функция возвращает ссылку, теги, заголовок новости, текст новости\n",
    "    \"\"\"\n",
    "    page = rq.get(url)\n",
    "    soup = BeautifulSoup(page.text, features=\"html.parser\")\n",
    "    \n",
    "    category = [] \n",
    "    for i in soup.find_all('a', class_= 'dcr-1gwziyt'):\n",
    "        category.append(i.text)\n",
    "    \n",
    "    category = ', '.join(category)          \n",
    "    title = soup.find('h1').text\n",
    "    text = ''\n",
    "    # На сайте постоянно обновляюься теги текста, в последний раз были следующие, но это было давно и сейчас они уже не работают\n",
    "    if soup.find_all('p', class_='dcr-1lpi6p1'):\n",
    "        text = [i.text for i in soup.find_all('p', class_='dcr-1lpi6p1')]\n",
    "    if soup.find_all('p', class_= 'dcr-epamsi'):\n",
    "        text = [i.text for i in soup.find_all('p', class_= 'dcr-epamsi')]\n",
    "    if soup.find_all('p', class_= 'dcr-vq85ex'):\n",
    "        text = [i.text for i in soup.find_all('p', class_= 'dcr-vq85ex')]\n",
    "    if soup.find_all('p', class_= 'dcr-ppzeq1'):\n",
    "        text = [i.text for i in soup.find_all('p', class_= 'dcr-ppzeq1')]\n",
    "    if soup.find_all('p', class_= 'dcr-1fp5gi9'):\n",
    "        text = [i.text for i in soup.find_all('p', class_= 'dcr-1fp5gi9')]\n",
    "    if soup.find_all('p', class_= 'dcr-1yimvw'):\n",
    "        text = [i.text for i in soup.find_all('p', class_= 'dcr-1yimvw')]    \n",
    "    \n",
    "    final_text = ' '.join(text)\n",
    "    final_text = final_text.replace('\\xa0', ' ').replace('\\n', '').strip()\n",
    "    \n",
    "    return url, category, title, final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_news(all_links):\n",
    "    list_of_news = []\n",
    "    \n",
    "    for link in tqdm(all_links):\n",
    "        try:\n",
    "            news = GetNews(link)\n",
    "            list_of_news.append(news)\n",
    "        except Exception as err:\n",
    "            print(f'Failed: {link}, error: {err}')\n",
    "            \n",
    "    return list_of_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links_climate = GetLinks(all_pages_climate)\n",
    "all_links_pollution = GetLinks(all_pages_pollution)\n",
    "all_links_energy = GetLinks(all_pages_energy)\n",
    "all_links_wildlife = GetLinks(all_pages_wildlife)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links_climate[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_climate = get_all_news(all_links_climate)\n",
    "news_pollution = get_all_news(all_links_pollution)\n",
    "news_energy = get_all_news(all_links_energy)\n",
    "news_wildlife = get_all_news(all_links_wildlife)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'No. of Climate News: {len(news_climate)}') \n",
    "print(f'No. of Pollution News: {len(news_pollution)}') \n",
    "print(f'No. of Energy News: {len(news_energy)}') \n",
    "print(f'No. of Wildlife News: {len(news_wildlife)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['link', 'category', 'title', 'text']\n",
    "\n",
    "df_climate = pd.DataFrame(news_climate, columns=columns)\n",
    "df_pollution = pd.DataFrame(news_pollution, columns=columns)\n",
    "df_energy = pd.DataFrame(news_energy, columns=columns)\n",
    "df_wildlife = pd.DataFrame(news_wildlife, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_environment = pd.concat([df_climate, df_pollution, df_energy, df_wildlife], ignore_index=True)\n",
    "df_environment = df_environment.drop_duplicates().reset_index(drop=True)\n",
    "df_environment = df_environment.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-regression",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-federation",
   "metadata": {},
   "source": [
    "# Препроцессинг данных и частотный анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nltk_download ('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stops.txt') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "for line in lines:\n",
    "    stop_words.append(line.strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = '!\\\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~—»«...–'\n",
    "filter = stop_words + list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_texts = df_environment['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = random.choices(full_texts, k=600) # новостный текстов оказалось очень много, я решила взять 600 из них для этого проекта\n",
    "combined_text = ''.join(list(chain.from_iterable(corpus)))\n",
    "\n",
    "# в некоторых текстах были обнаружены вставки с призывом подписаться, я их убрала:\n",
    "combined_text = combined_text.replace(\"Sign up for Guardian Australia’s free morning and afternoon email newsletters for your daily news roundup\", \"\")\n",
    "combined_text = combined_text.replace(\"Find more age of extinction coverage here, and follow biodiversity reporters Phoebe Weston and Patrick Greenfield on X for all the latest news and features\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotation_marks = '“”'\n",
    "combined_text = combined_text.translate(str.maketrans('', '', quotation_marks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text_2 = combined_text\n",
    "\n",
    "# Некоторые предложения после парсинга слиплись, тут я их разделяю:\n",
    "for combined in re.findall(r\"([a-z]+\\.[A-Z]+)\", combined_text):\n",
    "    correction = \". \".join(combined.split(\".\"))\n",
    "    combined_text_2 = combined_text_2.replace(combined, correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sentences = sent_tokenize(combined_text_2) # разбиваю корпус на отдельные предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentwise(input_sent):\n",
    "    \"\"\"\n",
    "    функция для предобработки текста, предварительно разбитого на предложения\n",
    "    \"\"\"\n",
    "    normalized = input_sent.lower()\n",
    "    doc = nlp(normalized)\n",
    "    words_lemmas = [token.lemma_ for token in doc]\n",
    "    no_stop_sent = [word for word in words_lemmas if word not in filter and word.isalpha()]\n",
    "    clean_sent = ' '.join(no_stop_sent)\n",
    "\n",
    "    return clean_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_orig_and_lemma = []\n",
    "\n",
    "for sent in tqdm(text_sentences):\n",
    "    lemma_sent = preprocess_sentwise(sent)\n",
    "    list_orig_and_lemma.append((sent, lemma_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig_and_lemma = pd.DataFrame(list_orig_and_lemma, columns=['original sentence', 'preprocessed sentence'])\n",
    "df_orig_and_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for sent in df_orig_and_lemma['original sentence'].values:\n",
    "    words.append(word_tokenize(sent))\n",
    "words_list = list(chain.from_iterable(words)) \n",
    "\n",
    "words_list_no_punkt = []\n",
    "for word in words_list:\n",
    "    if word not in punctuation: \n",
    "        words_list_no_punkt.append(word) \n",
    "        \n",
    "len(words_list_no_punkt) # размер корпуса (количество слов в корпусе до удаления стоп-слов, без знаков пунктуации)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for sent in df_orig_and_lemma['preprocessed sentence'].values:\n",
    "    tokens.append(word_tokenize(sent))\n",
    "tokens_list = list(chain.from_iterable(tokens)) # токены в корпусе после препроцессинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs = Counter(tokens_list) \n",
    "print(word_freqs.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1 = [element[0] for element in word_freqs.most_common(50)]\n",
    "counts_1 = [element[1] for element in word_freqs.most_common(50)]\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "plot = sns.barplot(x=labels_1, y=counts_1, ax=ax)\n",
    "ax.tick_params(labelrotation=90)\n",
    "ax.set_title(\"Самые частотные униграммы в корпусе\")\n",
    "ax.set_ylabel(\"Частота\")\n",
    "ax.set_xlabel(\"Биграммы\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_bigramms = Counter(nltk.bigrams(tokens_list))\n",
    "freq_bigramms.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_2 = [' '.join(element[0]) for element in freq_bigramms.most_common(30)]\n",
    "counts_2 = [element[1] for element in freq_bigramms.most_common(30)]\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "plot = sns.barplot(x=labels_2, y=counts_2, ax=ax)\n",
    "ax.set_title(\"Самые частотные биграммы в корпусе\")\n",
    "ax.set_ylabel(\"Частота\")\n",
    "ax.set_xlabel(\"Биграммы\")\n",
    "ax.tick_params(labelrotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-botswana",
   "metadata": {},
   "source": [
    "# Анализ тональности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_uni = []\n",
    "vocab_bi = []\n",
    "\n",
    "for word in word_freqs.most_common(100):\n",
    "    vocab_uni.append(word)\n",
    "for word in freq_bigramms.most_common(30):\n",
    "    vocab_bi.append(word)\n",
    "\n",
    "concatenated_vocab = vocab_uni + vocab_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sentiment = eng_spacysentiment.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_vocab = []\n",
    "\n",
    "for word in concatenated_vocab:\n",
    "    if len(word[0]) != 2:\n",
    "        keyword = word[0]\n",
    "    else:\n",
    "        keyword = ' '.join(word[0])\n",
    "    sents = df_orig_and_lemma[df_orig_and_lemma['preprocessed sentence'].str.contains(keyword)].values.tolist()\n",
    "\n",
    "    count_pos = 0\n",
    "    count_neg = 0\n",
    "\n",
    "    for org_sent, prep_sent in sents:\n",
    "        doc = nlp_sentiment(prep_sent)\n",
    "        sentiment = doc.cats\n",
    "    \n",
    "        if sentiment['positive'] > sentiment['negative']:\n",
    "            count_pos += 1\n",
    "        elif sentiment['negative'] > sentiment['positive']:\n",
    "            count_neg += 1\n",
    "    sentiment_vocab.append((keyword, word[1], count_pos, count_neg))\n",
    "\n",
    "vocab = pd.DataFrame(sentiment_vocab, columns=['keyword', 'count', 'count_pos', 'count_neg'])\n",
    "vocab\n",
    "\n",
    "#vocab.to_excel(\"vocab_trial.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"pollution\"\n",
    "\n",
    "sents = df_orig_and_lemma[df_orig_and_lemma['preprocessed sentence'].str.contains(keyword)].values.tolist()\n",
    "list_pos = []\n",
    "list_neg = []\n",
    "\n",
    "for org_sent, prep_sent in sents:\n",
    "    doc = nlp_sentiment(prep_sent)\n",
    "    sentiment = doc.cats\n",
    "    \n",
    "    if sentiment['positive'] > sentiment['negative']:\n",
    "        list_pos.append((org_sent, sentiment['positive']))\n",
    "    elif sentiment['negative'] > sentiment['positive']:\n",
    "        list_neg.append((org_sent, sentiment['negative'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод предложений, содержащих ключевое слово, для проверки результатов анализа тональности вручную, чтобы исправить данные в таблице Еxcel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Последующая работа велась в таблице Еxcel: \n",
    "# Там я считала PMI, T-score, хи-квадрат по формулам для отобранных ключевых слов\n",
    "# В ходе работы некоторые изначально отобранные частотные униграммы и биграммы были удалены, и добавлены другие менее частотные, но более релевантные"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
